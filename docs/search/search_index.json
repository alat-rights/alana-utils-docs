{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"alana-utilities (Utilities Library for LLM-Heavy Workflows)","text":""},{"location":"#install-instructions","title":"Install Instructions","text":"<pre><code>pip install alana\n</code></pre> <p>\ud83c\udfb5 Note: I have been making new releases frequently. Make sure your package is up-to-date!</p> <p>\u26a0\ufe0f Warning: This library is in active early development! No guarantees are made for backward compatibility. The library is NOT production-ready.</p>"},{"location":"#is-this-for-me","title":"Is this for me?","text":"<p>This library is mostly designed for me (hence the name), but they might be helpful for you too!</p> <p>Here are some questions to ask yourself when considering whether to use this library: - Do you interact with LLMs? (Currently, only Anthropic LLMs are supported). - Do you value conciseness? - Are you writing non-production prototype code, where 1, occassional bugs and behavioral changes are acceptable, and 2, developer ergonomics are more important than performance?   - Note: This library strongly assumes this use-case! e.g. <code>alana.gen</code> actually writes to the console by default (you can disable this with <code>loud=False</code>). - Do the features appeal to you?</p>"},{"location":"#philosophy","title":"Philosophy:","text":"<ul> <li>Programming is too slow! This is doubly true when you're interacting with LLMs. By building nice utilities with sane defaults, I hope to speed up my (and maybe your) workflow.</li> <li>I make trade-offs to speed up the developer experience:</li> <li>I do not try hard to anticipate future upstream API changes. I'm also ok with breaking backward compatibility to make my functions more concise and more usable.</li> <li>Usability &gt; Principles. While I don't relish in it, I'm ok with breaking conventions designed for large production libraries if it speeds up programmers who use <code>alana</code>. The priority is to make the library intuitive and fast.</li> <li>I don't try to serve every use-case.</li> <li>Simplicity is key. This library strives to be readable and straightforward.</li> </ul>"},{"location":"#motivating-examples","title":"Motivating Examples:","text":"<p>(I tested these, and tried to make sure my code was idiomatic in all cases. Sorry if I messed up! Please report any issues to <code>h i ( a t ) [pip package name for this library] dot computer</code>.)</p> <p>Continuing a list of messages using Anthropic API (adapted from Anthropic API documentation:</p> <pre><code>import anthropic\nfrom anthropic.types import MessageParam\nmessages = [\n  MessageParam(\n    role=\"user\",\n    content=\"Hello, Claude!\"\n  ),\n]\noutput_text = anthropic.Anthropic().messages.create(\n    model=\"claude-3-opus-20240229\",\n    max_tokens=1024,\n    messages=messages\n).content[0].text\nmessages.append(\n  MessageParam(\n    role=\"assistant\",\n    content=output_text\n  )\n)\nprint(messages)  # [{'role': 'user', 'content': 'Hello, Claude!'}, {'role': 'assistant', 'content': \"Hello! It's nice to meet you. How are you doing today?\"}]\n</code></pre> <p>Equivalent <code>alana</code> code:</p> <pre><code>import alana\nmessages = []\nalana.gen(user=\"Hello, Claude!\", messages=messages, model=\"opus\", max_tokens=1024)\nprint(messages)  # [{'role': 'user', 'content': 'Hello, Claude!'}, {'role': 'assistant', 'content': \"Hello! It's nice to meet you. How are you doing today?\"}]\n</code></pre> <p>Also, equivalent <code>alana</code> code thanks to defaults:</p> <pre><code>import alana\nmessages = []\nalana.gen(user=\"Hello, Claude!\", messages=messages)\nprint(messages)  # [{'role': 'user', 'content': 'Hello, Claude!'}, {'role': 'assistant', 'content': \"Hello! It's nice to meet you. How are you doing today?\"}]\n</code></pre>"},{"location":"#features","title":"Features:","text":"<ul> <li>Easy color print: <code>alana.red</code>, <code>alana.green</code>, <code>alana.blue</code>, <code>alana.yellow</code>, <code>alana.cyan</code>. Try <code>alana.green(\"Hello!\")</code></li> <li>Easy pretty print with Sonnet (or an Anthropic model of your choice): <code>alana.pretty_print</code>. Try <code>alana.pretty_print(t.arange(16, device='cpu').reshape(2,2,4))</code></li> <li>Make it easier to use the Anthropic API:</li> <li><code>alana.gen</code>, for easy Claude generations. Try <code>alana.gen(user=\"Hello, Claude!\")</code>. You can pass in a <code>messages</code> parameter either in place of or together with a <code>user</code> parameter. \u26a0\ufe0f <code>messages</code> support is not thoroughly tested yet!</li> <li><code>alana.gen_examples</code>, <code>alana.gen_examples_list</code> for generating few-shot examples.</li> <li><code>alana.gen_prompt</code>, for easy prompt generation (meta-prompt).</li> <li><code>alana.get_xml</code>, for using regex to get XML tag contents from model outputs. \u26a0\ufe0f Regex parsing of XML may be unreliable!</li> <li><code>alana.remove_xml</code> to strip certain XML tag-enclosed content from a string (along with the tags). This is primarily intended to get rid of \"...\" strings. \u26a0\ufe0f Regex parsing of XML may be unreliable! This function has not been thoroughly tested yet!</li> <li>A bunch of aliases (Try: <code>alana.few_shot</code>, <code>alana.n_shot</code>, or <code>alana.xml</code>)</li> </ul>"},{"location":"#coming-soon","title":"Coming Soon:","text":"<ul> <li>Improve docs</li> <li>Generating alternative prompts given a prompt</li> <li>Better support for multi-turn prompting</li> <li>OpenAI model support</li> <li>Support for automatic \"are you sure\"/\"are you confused\" multi-turn prompting</li> <li>Automatic error checking (are there mistakes in this code, sanity checking of model outputs)</li> <li>Automatic model-switching on rate limit</li> <li>Support for quick-and-dirty unit-testing with Claude!</li> <li>Prompt test case generation and easy prompt testing</li> </ul>"},{"location":"color/","title":"color","text":""},{"location":"color/#color","title":"<code>color</code>","text":""},{"location":"color/#color.blue","title":"<code>blue(var, loud=True, logger=None)</code>","text":"<p>Print the given variable in blue color using colorama and return the colored string.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>Any</code> <p>The variable to be printed in red color.</p> required <code>loud</code> <code>bool</code> <p>If True, print the colored output to the console. Defaults to True.</p> <code>True</code> <code>logger</code> <code>Optional[Logger]</code> <p>The logger to be used for logging. If provided, the colored output will be logged using this logger. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The input variable formatted as a red-colored string.</p> Source code in <code>alana/color.py</code> <pre><code>def blue(var: Any, loud: bool = True, logger: Optional[logging.Logger] = None) -&gt; str:\n    \"\"\"\n    Print the given variable in blue color using colorama and return the colored string.\n\n    Args:\n        var (Any): The variable to be printed in red color.\n        loud (bool, optional): If True, print the colored output to the console. Defaults to True.\n        logger (Optional[logging.Logger], optional): The logger to be used for logging.\n            If provided, the colored output will be logged using this logger. Defaults to None.\n\n    Returns:\n        str: The input variable formatted as a red-colored string.\n    \"\"\"\n    output = f\"{Fore.BLUE} {var} {Style.RESET_ALL}\"\n    log(loud, output, logger)\n    return output\n</code></pre>"},{"location":"color/#color.cyan","title":"<code>cyan(var, loud=True, logger=None)</code>","text":"<p>Print var in cyan, like <code>alana.blue</code></p> Source code in <code>alana/color.py</code> <pre><code>def cyan(var: Any, loud: bool = True, logger: Optional[logging.Logger] = None) -&gt; str:\n    \"\"\"Print var in cyan, like `alana.blue`\"\"\"\n    output = f\"{Fore.CYAN} {var} {Style.RESET_ALL}\"\n    log(loud, output, logger)\n    return output\n</code></pre>"},{"location":"color/#color.green","title":"<code>green(var, loud=True, logger=None)</code>","text":"<p>Print var in green, like <code>alana.blue</code></p> Source code in <code>alana/color.py</code> <pre><code>def green(var: Any, loud: bool = True, logger: Optional[logging.Logger] = None) -&gt; str:\n    \"\"\"Print var in green, like `alana.blue`\"\"\"\n    output = f\"{Fore.GREEN} {var} {Style.RESET_ALL}\"\n    log(loud, output, logger)\n    return output\n</code></pre>"},{"location":"color/#color.log","title":"<code>log(loud, output, logger=None)</code>","text":"<p>Log the output based on the provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>loud</code> <code>bool</code> <p>If True, print the output to the console.</p> required <code>output</code> <code>str</code> <p>The string to be logged.</p> required <code>logger</code> <code>Optional[Logger]</code> <p>The logger to be used for logging. If provided, the output will be logged using this logger. Defaults to None.</p> <code>None</code> Source code in <code>alana/color.py</code> <pre><code>def log(loud: bool, output: str, logger: Optional[logging.Logger] = None) -&gt; None:\n    \"\"\"\n    Log the output based on the provided parameters.\n\n    Args:\n        loud (bool): If True, print the output to the console.\n        output (str): The string to be logged.\n        logger (Optional[logging.Logger], optional): The logger to be used for logging.\n            If provided, the output will be logged using this logger. Defaults to None.\n    \"\"\"\n    if loud:\n        print(output)\n    if logger:\n        logger.info(output)\n</code></pre>"},{"location":"color/#color.pink","title":"<code>pink(var, loud=True, logger=None)</code>","text":"<p>Print var in pink, like <code>alana.blue</code></p> Source code in <code>alana/color.py</code> <pre><code>def pink(var: Any, loud: bool = True, logger: Optional[logging.Logger] = None) -&gt; str:\n    \"\"\"Print var in pink, like `alana.blue`\"\"\"\n    output = f\"{Fore.MAGENTA} {var} {Style.RESET_ALL}\"\n    log(loud, output, logger)\n    return output\n</code></pre>"},{"location":"color/#color.red","title":"<code>red(var, loud=True, logger=None)</code>","text":"<p>Print var in red, like <code>alana.blue</code></p> Source code in <code>alana/color.py</code> <pre><code>def red(var: Any, loud: bool = True, logger: Optional[logging.Logger] = None) -&gt; str:\n    \"\"\"Print var in red, like `alana.blue`\"\"\"\n    output = f\"{Fore.RED} {var} {Style.RESET_ALL}\"\n    log(loud, output, logger)\n    return output\n</code></pre>"},{"location":"color/#color.yellow","title":"<code>yellow(var, loud=True, logger=None)</code>","text":"<p>Print var in yellow, like <code>alana.blue</code></p> Source code in <code>alana/color.py</code> <pre><code>def yellow(var: Any, loud: bool = True, logger: Optional[logging.Logger] = None) -&gt; str:\n    \"\"\"Print var in yellow, like `alana.blue`\"\"\"\n    output = f\"{Fore.YELLOW} {var} {Style.RESET_ALL}\"\n    log(loud, output, logger)\n    return output\n</code></pre>"},{"location":"prompt/","title":"prompt","text":""},{"location":"prompt/#prompt","title":"<code>prompt</code>","text":""},{"location":"prompt/#prompt.few_shot","title":"<code>few_shot(instruction, n_examples=5, model=globals.DEFAULT_MODEL, api_key=None, max_tokens=1024, temperature=0.3, **kwargs)</code>","text":"<p>Alias for gen_examples</p> Source code in <code>alana/prompt.py</code> <pre><code>def few_shot(instruction: str, n_examples: int = 5, model: str = globals.DEFAULT_MODEL, api_key: Optional[str] = None, max_tokens: int = 1024, temperature=0.3, **kwargs) -&gt; str:\n    \"\"\"Alias for gen_examples\"\"\"\n    return gen_examples(instruction=instruction, n_examples=n_examples, model=model, api_key=api_key, max_tokens=max_tokens, temperature=temperature, **kwargs)\n</code></pre>"},{"location":"prompt/#prompt.few_shot_list","title":"<code>few_shot_list(instruction, n_examples=5, model=globals.DEFAULT_MODEL, api_key=None, max_tokens=1024, temperature=0.3, **kwargs)</code>","text":"<p>Alias for gen_examples_list</p> Source code in <code>alana/prompt.py</code> <pre><code>def few_shot_list(instruction: str, n_examples: int = 5, model: str = globals.DEFAULT_MODEL, api_key: Optional[str] = None, max_tokens: int = 1024, temperature=0.3, **kwargs) -&gt; List[str]:\n    \"\"\"Alias for gen_examples_list\"\"\"\n    return gen_examples_list(instruction=instruction, n_examples=n_examples, model=model, api_key=api_key, max_tokens=max_tokens, temperature=temperature, **kwargs)\n</code></pre>"},{"location":"prompt/#prompt.gen","title":"<code>gen(user=None, system='', messages=None, append=True, model=globals.DEFAULT_MODEL, api_key=None, max_tokens=1024, temperature=0.3, loud=True, **kwargs)</code>","text":"<p>Generate a response from Claude. Returns the text content (<code>str</code>) of Claude's response. If you want the Message object instead, use <code>gen_msg</code>.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>Optional[str]</code> <p>The user's message content. Defaults to None.</p> <code>None</code> <code>system</code> <code>str</code> <p>The system message for Claude. Defaults to \"\".</p> <code>''</code> <code>messages</code> <code>Optional[List[MessageParam]]</code> <p>A list of <code>anthropic.types.MessageParam</code>. Defaults to None.</p> <code>None</code> <code>append</code> <code>bool</code> <p>Whether to append the generated response (as an <code>anthropic.types.MessageParam</code>) to <code>messages</code>. Defaults to True.</p> <code>True</code> <code>model</code> <code>str</code> <p>The name of the model to use. Defaults to globals.DEFAULT_MODEL.</p> <code>DEFAULT_MODEL</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for authentication. Defaults to None (if None, uses os.environ[\"ANTHROPIC_API_KEY]).</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate in the response. Defaults to 1024.</p> <code>1024</code> <code>temperature</code> <code>float</code> <p>The temperature value for controlling the randomness of the generated response. Defaults to 0.3.</p> <code>0.3</code> <code>loud</code> <code>bool</code> <p>Whether to print verbose output. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the underlying generation function.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no prompt is provided (both <code>user</code> and <code>messages</code> are None).</p> <code>ValueError</code> <p>If the last message in <code>messages</code> is from the user and <code>user</code> is also provided.</p> <code>ValueError</code> <p>If Claude does not provide a response.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text content of Claude's generated response.</p> Notes <ul> <li>If <code>messages</code> is None, the <code>user</code> parameter must be provided as a string.</li> <li>If <code>user</code> is provided and <code>messages</code> is not None, the <code>user</code> message is appended to the <code>messages</code> list.</li> <li>The function raises a ValueError if the roles in the <code>messages</code> list are not alternating (e.g., user, assistant, user).</li> <li>If <code>append</code> is True and the last message in <code>messages</code> is from the assistant, the generated response is appended to the existing assistant's content.</li> <li>The function uses the <code>gen_msg</code> function internally to generate Claude's response.</li> </ul> Example <p>user_message = \"Hello, Claude!\" response = gen(user=user_message) print(response) \"Hello! How can I assist you today?\"</p> Source code in <code>alana/prompt.py</code> <pre><code>def gen(user: Optional[str] = None, system: str = \"\", messages: Optional[List[MessageParam]] = None, append: bool = True, model: str = globals.DEFAULT_MODEL, api_key: Optional[str] = None, max_tokens = 1024, temperature=0.3, loud=True, **kwargs) -&gt; str:\n    \"\"\"Generate a response from Claude. Returns the text content (`str`) of Claude's response. If you want the Message object instead, use `gen_msg`.\n\n    Args:\n        user (Optional[str], optional): The user's message content. Defaults to None.\n        system (str, optional): The system message for Claude. Defaults to \"\".\n        messages (Optional[List[MessageParam]], optional): A list of `anthropic.types.MessageParam`. Defaults to None.\n        append (bool, optional): Whether to append the generated response (as an `anthropic.types.MessageParam`) to `messages`. Defaults to True.\n        model (str, optional): The name of the model to use. Defaults to globals.DEFAULT_MODEL.\n        api_key (Optional[str], optional): The API key to use for authentication. Defaults to None (if None, uses os.environ[\"ANTHROPIC_API_KEY]).\n        max_tokens (int, optional): The maximum number of tokens to generate in the response. Defaults to 1024.\n        temperature (float, optional): The temperature value for controlling the randomness of the generated response. Defaults to 0.3.\n        loud (bool, optional): Whether to print verbose output. Defaults to True.\n        **kwargs: Additional keyword arguments to pass to the underlying generation function.\n\n    Raises:\n        ValueError: If no prompt is provided (both `user` and `messages` are None).\n        ValueError: If the last message in `messages` is from the user and `user` is also provided.\n        ValueError: If Claude does not provide a response.\n\n    Returns:\n        str: The text content of Claude's generated response.\n\n    Notes:\n        - If `messages` is None, the `user` parameter must be provided as a string.\n        - If `user` is provided and `messages` is not None, the `user` message is appended to the `messages` list.\n        - The function raises a ValueError if the roles in the `messages` list are not alternating (e.g., user, assistant, user).\n        - If `append` is True and the last message in `messages` is from the assistant, the generated response is appended to the existing assistant's content.\n        - The function uses the `gen_msg` function internally to generate Claude's response.\n\n    Example:\n        &gt;&gt;&gt; user_message = \"Hello, Claude!\"\n        &gt;&gt;&gt; response = gen(user=user_message)\n        &gt;&gt;&gt; print(response)\n        \"Hello! How can I assist you today?\"\n    \"\"\"\n    if user is None and messages is None:\n        raise ValueError(\"No prompt provided! `user` and `messages` are both None.\")\n\n    if messages is None:\n        assert user is not None  # To be stricter, type(user) == str\n        messages=[\n            MessageParam(role=\"user\", content=user), # type: ignore\n        ]\n    elif user is not None:\n        assert messages is not None  # To be stricter, messages is List[MessageParam]\n        if len(messages) &gt;= 1 and messages[-1][\"role\"] == \"user\":\n            raise ValueError(\"`gen`: Bad request! Roles must be alternating. Last message in `messages` is from user, but `user` provided.\")\n        messages.append(\n            MessageParam(role=\"user\", content=user)  # TODO: Check that non-alternating roles are ok (e.g. user, assistant, assistant)\n        )\n\n    output: Message = gen_msg(system=system, messages=messages, model=model, api_key=api_key, max_tokens=max_tokens, temperature=temperature, **kwargs)\n\n    if len(output.content) == 0:\n        raise ValueError(f\"Claude did not provide a response. Stop reason: {output.stop_reason}. Full API response: {output}\")\n\n    if append == True:\n        if messages[-1][\"role\"] == \"assistant\":  # NOTE: Anthropic API does not allow non-alternating roles (raises Err400). Let's enforce this.\n            existing_assistant_content: str = messages[-1][\"role\"]\n            assistant_content: str = existing_assistant_content + output.content[0].text\n            messages.pop()\n        else:\n            assistant_content: str = output.content[0].text\n\n        messages.append(\n            MessageParam(\n                role=\"assistant\",\n                content=assistant_content\n            )\n        )\n    return output.content[0].text\n</code></pre>"},{"location":"prompt/#prompt.gen_examples","title":"<code>gen_examples(instruction, n_examples=5, model=globals.DEFAULT_MODEL, api_key=None, max_tokens=1024, temperature=0.3, **kwargs)</code>","text":"<p>Generate a formatted string containing few-shot examples for a given natural language instruction. Uses <code>gen_examples_list</code>.</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <code>str</code> <p>The natural language instruction for which to generate examples.</p> required <code>n_examples</code> <code>int</code> <p>The number of examples to generate. Defaults to 5.</p> <code>5</code> <code>model</code> <code>str</code> <p>The name of the model to use. Defaults to globals.DEFAULT_MODEL.</p> <code>DEFAULT_MODEL</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for authentication. Defaults to None.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate in the response. Defaults to 1024.</p> <code>1024</code> <code>temperature</code> <code>float</code> <p>The temperature value for controlling the randomness of the generated response. Defaults to 0.3.</p> <code>0.3</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>gen_examples_list</code> function (passed to Anthropic API).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted string containing the generated few-shot examples, enclosed in XML-like tags.</p> Notes <ul> <li>The function calls the <code>gen_examples_list</code> function to generate a list of few-shot examples based on the provided <code>instruction</code>, <code>n_examples</code>, <code>model</code>, <code>api_key</code>, <code>max_tokens</code>, <code>temperature</code>, and any additional keyword arguments.</li> <li>The generated examples are then formatted into a string, with each example enclosed in <code>&lt;example/&gt;</code> tags.</li> <li>The formatted string starts with an opening <code>&lt;examples&gt;</code> tag and ends with a closing <code>&lt;/examples&gt;</code> tag (note plural).</li> </ul> Example <p>instruction = \"Write a short story about a magical adventure.\" examples_str = gen_examples(instruction, n_examples=3) print(examples_str)  Once upon a time, in a land far away, there was a young girl named Lily who discovered a mysterious portal in her backyard... In a world where magic was a part of everyday life, a brave knight named Eldric embarked on a quest to retrieve a powerful artifact... Deep in the enchanted forest, a group of talking animals gathered around a wise old oak tree to discuss a pressing matter... </p> Source code in <code>alana/prompt.py</code> <pre><code>def gen_examples(instruction: str, n_examples: int = 5, model: str = globals.DEFAULT_MODEL, api_key: Optional[str] = None, max_tokens: int = 1024, temperature=0.3, **kwargs) -&gt; str:\n    \"\"\"Generate a formatted string containing few-shot examples for a given natural language instruction. Uses `gen_examples_list`.\n\n    Args:\n        instruction (str): The natural language instruction for which to generate examples.\n        n_examples (int, optional): The number of examples to generate. Defaults to 5.\n        model (str, optional): The name of the model to use. Defaults to globals.DEFAULT_MODEL.\n        api_key (Optional[str], optional): The API key to use for authentication. Defaults to None.\n        max_tokens (int, optional): The maximum number of tokens to generate in the response. Defaults to 1024.\n        temperature (float, optional): The temperature value for controlling the randomness of the generated response. Defaults to 0.3.\n        **kwargs: Additional keyword arguments to pass to the `gen_examples_list` function (passed to Anthropic API).\n\n    Returns:\n        str: A formatted string containing the generated few-shot examples, enclosed in XML-like tags.\n\n    Notes:\n        - The function calls the `gen_examples_list` function to generate a list of few-shot examples based on the provided `instruction`, `n_examples`, `model`, `api_key`, `max_tokens`, `temperature`, and any additional keyword arguments.\n        - The generated examples are then formatted into a string, with each example enclosed in `&lt;example/&gt;` tags.\n        - The formatted string starts with an opening `&lt;examples&gt;` tag and ends with a closing `&lt;/examples&gt;` tag (note plural).\n\n    Example:\n        &gt;&gt;&gt; instruction = \"Write a short story about a magical adventure.\"\n        &gt;&gt;&gt; examples_str = gen_examples(instruction, n_examples=3)\n        &gt;&gt;&gt; print(examples_str)\n        &lt;examples&gt;\n        &lt;example&gt;Once upon a time, in a land far away, there was a young girl named Lily who discovered a mysterious portal in her backyard...&lt;/example&gt;\n        &lt;example&gt;In a world where magic was a part of everyday life, a brave knight named Eldric embarked on a quest to retrieve a powerful artifact...&lt;/example&gt;\n        &lt;example&gt;Deep in the enchanted forest, a group of talking animals gathered around a wise old oak tree to discuss a pressing matter...&lt;/example&gt;\n        &lt;/examples&gt;\n    \"\"\"\n    examples: List[str] = gen_examples_list(instruction=instruction, n_examples=n_examples, model=model, api_key=api_key, max_tokens=max_tokens, temperature=temperature, **kwargs)\n    formatted_examples: str = \"\\n&lt;examples&gt;\\n&lt;example&gt;\" + '&lt;/example&gt;\\n&lt;example&gt;'.join(examples) + \"&lt;/example&gt;\\n&lt;/examples&gt;\"\n    return formatted_examples\n</code></pre>"},{"location":"prompt/#prompt.gen_examples_list","title":"<code>gen_examples_list(instruction, n_examples=5, model=globals.DEFAULT_MODEL, api_key=None, max_tokens=1024, temperature=0.3, **kwargs)</code>","text":"<p>Uses Claude to generate a Python list of few-shot examples for a given natural language instruction.</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <code>str</code> <p>The natural language instruction for which to generate examples.</p> required <code>n_examples</code> <code>int</code> <p>The number of examples to ask Claude to generate. Defaults to 5.</p> <code>5</code> <code>model</code> <code>str</code> <p>The name of the model to use. Defaults to <code>globals.DEFAULT_MODEL</code>.</p> <code>DEFAULT_MODEL</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for authentication. Defaults to None.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate in the response. Defaults to 1024.</p> <code>1024</code> <code>temperature</code> <code>float</code> <p>The temperature value for controlling the randomness of the generated response. Defaults to 0.3.</p> <code>0.3</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>gen</code> function (<code>gen</code> passes kwargs to the Anthropic API).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A Python list of generated few-shot examples.</p> Notes <ul> <li>The function constructs a system message using the <code>globals.SYSTEM[\"few_shot\"]</code> template and the provided <code>n_examples</code>.</li> <li>The function constructs a user message using the <code>globals.USER[\"few_shot\"]</code> template and the provided <code>instruction</code>.</li> <li>If <code>n_examples</code> is less than 1, the function prints a warning message using the <code>red</code> function but continues execution.</li> <li>The function calls the <code>gen</code> function to generate the model's output based on the constructed system and user messages, along with the specified <code>model</code>, <code>api_key</code>, <code>max_tokens</code>, <code>temperature</code>, and any additional keyword arguments.</li> <li>The generated model output is expected to be in XML format, with each example enclosed in <code>&lt;example/&gt;</code> tags.</li> <li>The function uses the <code>get_xml</code> function to extract the content within the <code>&lt;example/&gt;</code> tags and returns it as a Python list of strings.</li> </ul> Example <p>instruction = \"Write a short story about a magical adventure.\" examples = gen_examples_list(instruction, n_examples=3) print(examples) [     \"Once upon a time, in a land far away, there was a young girl named Lily who discovered a mysterious portal in her backyard...\",     \"In a world where magic was a part of everyday life, a brave knight named Eldric embarked on a quest to retrieve a powerful artifact...\",     \"Deep in the enchanted forest, a group of talking animals gathered around a wise old oak tree to discuss a pressing matter...\" ]</p> Source code in <code>alana/prompt.py</code> <pre><code>def gen_examples_list(instruction: str, n_examples: int = 5, model: str = globals.DEFAULT_MODEL, api_key: Optional[str] = None, max_tokens: int = 1024, temperature=0.3, **kwargs) -&gt; List[str]:\n    \"\"\"Uses Claude to generate a Python list of few-shot examples for a given natural language instruction.\n\n    Args:\n        instruction (str): The natural language instruction for which to generate examples.\n        n_examples (int, optional): The number of examples to ask Claude to generate. Defaults to 5.\n        model (str, optional): The name of the model to use. Defaults to `globals.DEFAULT_MODEL`.\n        api_key (Optional[str], optional): The API key to use for authentication. Defaults to None.\n        max_tokens (int, optional): The maximum number of tokens to generate in the response. Defaults to 1024.\n        temperature (float, optional): The temperature value for controlling the randomness of the generated response. Defaults to 0.3.\n        **kwargs: Additional keyword arguments to pass to the `gen` function (`gen` passes kwargs to the Anthropic API).\n\n    Returns:\n        List[str]: A Python list of generated few-shot examples.\n\n    Notes:\n        - The function constructs a system message using the `globals.SYSTEM[\"few_shot\"]` template and the provided `n_examples`.\n        - The function constructs a user message using the `globals.USER[\"few_shot\"]` template and the provided `instruction`.\n        - If `n_examples` is less than 1, the function prints a warning message using the `red` function but continues execution.\n        - The function calls the `gen` function to generate the model's output based on the constructed system and user messages, along with the specified `model`, `api_key`, `max_tokens`, `temperature`, and any additional keyword arguments.\n        - The generated model output is expected to be in XML format, with each example enclosed in `&lt;example/&gt;` tags.\n        - The function uses the `get_xml` function to extract the content within the `&lt;example/&gt;` tags and returns it as a Python list of strings.\n\n    Example:\n        &gt;&gt;&gt; instruction = \"Write a short story about a magical adventure.\"\n        &gt;&gt;&gt; examples = gen_examples_list(instruction, n_examples=3)\n        &gt;&gt;&gt; print(examples)\n        [\n            \"Once upon a time, in a land far away, there was a young girl named Lily who discovered a mysterious portal in her backyard...\",\n            \"In a world where magic was a part of everyday life, a brave knight named Eldric embarked on a quest to retrieve a powerful artifact...\",\n            \"Deep in the enchanted forest, a group of talking animals gathered around a wise old oak tree to discuss a pressing matter...\"\n        ]\n    \"\"\"\n    system: str = globals.SYSTEM[\"few_shot\"].format(n_examples=n_examples)\n    user: str = globals.USER[\"few_shot\"].format(instruction=instruction)\n    if n_examples &lt; 1:\n        red(var=\"Too few examples provided! Trying anyway...\")\n\n    model_output: str = gen(user=user, system=system, model=model, api_key=api_key, max_tokens=max_tokens, temperature=temperature, **kwargs)\n    return get_xml(tag='example', content=model_output)\n</code></pre>"},{"location":"prompt/#prompt.gen_msg","title":"<code>gen_msg(messages, system='', model=globals.DEFAULT_MODEL, api_key=None, max_tokens=1024, temperature=0.3, loud=True, **kwargs)</code>","text":"<p>Generate a response from Claude using the Anthropic API.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[MessageParam]</code> <p>A list of <code>anthropic.types.MessageParam</code>s representing the conversation history.</p> required <code>system</code> <code>str</code> <p>The system message to set the context for Claude. Defaults to \"\".</p> <code>''</code> <code>model</code> <code>str</code> <p>The name of the model to use. Defaults to globals.DEFAULT_MODEL.</p> <code>DEFAULT_MODEL</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for authentication. Defaults to None.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate in the response. Defaults to 1024.</p> <code>1024</code> <code>temperature</code> <code>float</code> <p>The temperature value for controlling the randomness of the generated response. Defaults to 0.3.</p> <code>0.3</code> <code>loud</code> <code>bool</code> <p>Whether to print verbose output. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the Anthropic API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Message</code> <code>Message</code> <p>The Message object produced by the Anthropic API, containing the generated response.</p> Notes <ul> <li>If the <code>model</code> parameter is not recognized, the function reverts to using the default model specified in <code>globals.DEFAULT_MODEL</code>.</li> <li>If <code>api_key</code> is None, the function attempts to retrieve the API key from the environment variable \"ANTHROPIC_API_KEY\".</li> <li>The function creates an instance of the Anthropic client using the provided <code>api_key</code>.</li> <li>Stream not supported yet! If the <code>stream</code> keyword argument is provided, the function disables streaming and sets <code>stream</code> to False. (TODO: Support stream)</li> <li>The function uses the <code>messages.create</code> method of the Anthropic client to generate Claude's response.</li> <li>If <code>loud</code> is True, the generated message is printed using the <code>yellow</code> function for verbose output.</li> </ul> Example <p>messages = [ ...     MessageParam(role=\"user\", content=\"What is the capital of France?\") ... ] response = gen_msg(messages, system=\"You are a helpful assistant.\") print(response.content[0].text) The capital of France is Paris.</p> Source code in <code>alana/prompt.py</code> <pre><code>def gen_msg(messages: List[MessageParam], system: str = \"\", model: str = globals.DEFAULT_MODEL, api_key: Optional[str] = None, max_tokens = 1024, temperature=0.3, loud=True, **kwargs) -&gt; Message:\n    \"\"\"Generate a response from Claude using the Anthropic API.\n\n    Args:\n        messages (List[MessageParam]): A list of `anthropic.types.MessageParam`s representing the conversation history.\n        system (str, optional): The system message to set the context for Claude. Defaults to \"\".\n        model (str, optional): The name of the model to use. Defaults to globals.DEFAULT_MODEL.\n        api_key (Optional[str], optional): The API key to use for authentication. Defaults to None.\n        max_tokens (int, optional): The maximum number of tokens to generate in the response. Defaults to 1024.\n        temperature (float, optional): The temperature value for controlling the randomness of the generated response. Defaults to 0.3.\n        loud (bool, optional): Whether to print verbose output. Defaults to True.\n        **kwargs: Additional keyword arguments to pass to the Anthropic API.\n\n    Returns:\n        Message: The Message object produced by the Anthropic API, containing the generated response.\n\n    Notes:\n        - If the `model` parameter is not recognized, the function reverts to using the default model specified in `globals.DEFAULT_MODEL`.\n        - If `api_key` is None, the function attempts to retrieve the API key from the environment variable \"ANTHROPIC_API_KEY\".\n        - The function creates an instance of the Anthropic client using the provided `api_key`.\n        - Stream not supported yet! If the `stream` keyword argument is provided, the function disables streaming and sets `stream` to False. (TODO: Support stream)\n        - The function uses the `messages.create` method of the Anthropic client to generate Claude's response.\n        - If `loud` is True, the generated message is printed using the `yellow` function for verbose output.\n\n    Example:\n        &gt;&gt;&gt; messages = [\n        ...     MessageParam(role=\"user\", content=\"What is the capital of France?\")\n        ... ]\n        &gt;&gt;&gt; response = gen_msg(messages, system=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; print(response.content[0].text)\n        The capital of France is Paris.\n    \"\"\"\n    backend: str = globals.MODELS[globals.DEFAULT_MODEL]\n    if model in globals.MODELS:\n        backend = globals.MODELS[model]\n    else:\n        red(var=f\"gen() -- Caution! model string not recognized; reverting to {globals.DEFAULT_MODEL=}.\") # TODO: C'mon we can do better error logging than this\n\n    if api_key is None:\n        api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n    client = Anthropic(\n        api_key=api_key,\n    )\n\n    if 'stream' in kwargs:\n        red(var=\"Streaming not supported! Disabling...\")\n        kwargs['stream'] = False\n\n    message: Message = client.messages.create(  # TODO: Enable streaming support\n        max_tokens=max_tokens,\n        messages=messages,\n        system=system,\n        model=backend,\n        temperature=temperature,\n        **kwargs\n    )\n    if loud:\n        yellow(var=message)\n\n    return message\n</code></pre>"},{"location":"prompt/#prompt.gen_prompt","title":"<code>gen_prompt(instruction, model=globals.DEFAULT_MODEL, api_key=None, max_tokens=1024, temperature=0.3, **kwargs)</code>","text":"<p>Meta-prompter! Generate a prompt given an arbitrary instruction.</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <code>str</code> <p>The arbitrary instruction for which to generate a prompt.</p> required <code>model</code> <code>str</code> <p>The name of the model to use. Defaults to globals.DEFAULT_MODEL.</p> <code>DEFAULT_MODEL</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for authentication. Defaults to None.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate in the response. Defaults to 1024.</p> <code>1024</code> <code>temperature</code> <code>float</code> <p>The temperature value for controlling the randomness of the generated response. Defaults to 0.3.</p> <code>0.3</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>gen</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[Literal['system', 'user', 'full'], Union[str, List]]</code> <p>Dict[Literal[\"system\", \"user\", \"full\"], Union[str, List]]: A dictionary containing the generated prompts. - \"system\" (Union[str, List[str]]): The generated system prompt(s). - \"user\" (Union[str, List[str]]): The generated user prompt(s). - \"full\" (str): The full generated output, including both system and user prompts.</p> Notes <ul> <li>The function constructs a meta-system prompt using the <code>globals.SYSTEM[\"gen_prompt\"]</code> template.</li> <li>The function constructs a meta-prompt using the <code>globals.USER[\"gen_prompt\"]</code> template and the provided <code>instruction</code>.</li> <li>The function calls the <code>gen</code> function to generate the full output based on the meta-system prompt, meta-prompt, <code>model</code>, <code>api_key</code>, <code>max_tokens</code>, <code>temperature</code>, and any additional keyword arguments (which are passed to the Anthropic API).</li> <li>The function uses the <code>get_xml</code> function to extract the content within the <code>&lt;system_prompt/&gt;</code> and <code>&lt;user_prompt/&gt;</code> tags from the full output.</li> <li>The function returns a dictionary containing the generated system prompt(s), user prompt(s), and the full output.</li> <li>Things can get janky if the model tries to provide multiple system prompts or multiple user prompts. I make some wild guess about what you might want to get in that case (right now, it would return the first system prompt, but all the user prompts in a list).</li> </ul> Example <p>instruction = \"Write a story about a robot learning to love.\" prompts = gen_prompt(instruction) print(prompts[\"system\"]) You are a creative story writer. Write a short story based on the given prompt, focusing on character development and emotional depth. print(prompts[\"user\"]) Write a story about a robot learning to love. print(prompts[\"full\"])  You are a creative story writer. Write a short story based on the given prompt, focusing on character development and emotional depth. </p> <p> Write a story about a robot learning to love. </p> Source code in <code>alana/prompt.py</code> <pre><code>def gen_prompt(instruction: str, model: str = globals.DEFAULT_MODEL, api_key: Optional[str] = None, max_tokens: int = 1024, temperature=0.3, **kwargs) -&gt; Dict[Literal[\"system\", \"user\", \"full\"], Union[str, List]]:\n    \"\"\"Meta-prompter! Generate a prompt given an arbitrary instruction.\n\n    Args:\n        instruction (str): The arbitrary instruction for which to generate a prompt.\n        model (str, optional): The name of the model to use. Defaults to globals.DEFAULT_MODEL.\n        api_key (Optional[str], optional): The API key to use for authentication. Defaults to None.\n        max_tokens (int, optional): The maximum number of tokens to generate in the response. Defaults to 1024.\n        temperature (float, optional): The temperature value for controlling the randomness of the generated response. Defaults to 0.3.\n        **kwargs: Additional keyword arguments to pass to the `gen` function.\n\n    Returns:\n        Dict[Literal[\"system\", \"user\", \"full\"], Union[str, List]]: A dictionary containing the generated prompts.\n            - \"system\" (Union[str, List[str]]): The generated system prompt(s).\n            - \"user\" (Union[str, List[str]]): The generated user prompt(s).\n            - \"full\" (str): The full generated output, including both system and user prompts.\n\n    Notes:\n        - The function constructs a meta-system prompt using the `globals.SYSTEM[\"gen_prompt\"]` template.\n        - The function constructs a meta-prompt using the `globals.USER[\"gen_prompt\"]` template and the provided `instruction`.\n        - The function calls the `gen` function to generate the full output based on the meta-system prompt, meta-prompt, `model`, `api_key`, `max_tokens`, `temperature`, and any additional keyword arguments (which are passed to the Anthropic API).\n        - The function uses the `get_xml` function to extract the content within the `&lt;system_prompt/&gt;` and `&lt;user_prompt/&gt;` tags from the full output.\n        - The function returns a dictionary containing the generated system prompt(s), user prompt(s), and the full output.\n        - Things can get janky if the model tries to provide multiple system prompts or multiple user prompts. I make some wild guess about what you might want to get in that case (right now, it would return the first system prompt, but all the user prompts in a list).\n\n    Example:\n        &gt;&gt;&gt; instruction = \"Write a story about a robot learning to love.\"\n        &gt;&gt;&gt; prompts = gen_prompt(instruction)\n        &gt;&gt;&gt; print(prompts[\"system\"])\n        You are a creative story writer. Write a short story based on the given prompt, focusing on character development and emotional depth.\n        &gt;&gt;&gt; print(prompts[\"user\"])\n        Write a story about a robot learning to love.\n        &gt;&gt;&gt; print(prompts[\"full\"])\n        &lt;system_prompt&gt;\n        You are a creative story writer. Write a short story based on the given prompt, focusing on character development and emotional depth.\n        &lt;/system_prompt&gt;\n\n        &lt;user_prompt&gt;\n        Write a story about a robot learning to love.\n        &lt;/user_prompt&gt;\n    \"\"\"\n    meta_system_prompt: str = globals.SYSTEM[\"gen_prompt\"]\n    meta_prompt: str = globals.USER[\"gen_prompt\"].format(instruction=instruction)\n\n    full_output: str = gen(user=meta_prompt, system=meta_system_prompt, model=model, api_key=api_key, max_tokens=max_tokens, temperature=temperature, **kwargs)\n    system_prompt: Union[List[str], str] = get_xml(tag=\"system_prompt\", content=full_output)\n    if len(system_prompt) &gt;= 1:\n        system_prompt = system_prompt[0]\n    user_prompt: Union[List[str], str] = get_xml(tag=\"user_prompt\", content=full_output)\n    if len(user_prompt) == 1:  # TODO: Find a saner way to handle this. E.g. delegate to a formatter model.\n        user_prompt = user_prompt[0]\n    return {\"system\": system_prompt, \"user\": user_prompt, \"full\": full_output}\n</code></pre>"},{"location":"prompt/#prompt.get_xml","title":"<code>get_xml(tag, content)</code>","text":"<p>Return contents of  XML tags.</p> Source code in <code>alana/prompt.py</code> <pre><code>def get_xml(tag: str, content: str) -&gt; List[str]:\n    \"\"\"Return contents of &lt;tag/&gt; XML tags.\"\"\"\n    pattern: str = get_xml_pattern(tag=tag)\n    matches: List[Any] = re.findall(pattern=pattern, string=content, flags=re.DOTALL)\n    return matches\n</code></pre>"},{"location":"prompt/#prompt.get_xml_pattern","title":"<code>get_xml_pattern(tag)</code>","text":"<p>Return regex pattern for getting contents of  XML tags.</p> Source code in <code>alana/prompt.py</code> <pre><code>def get_xml_pattern(tag: str):\n    \"\"\"Return regex pattern for getting contents of &lt;tag/&gt; XML tags.\"\"\"\n    if tag.count('&lt;') &gt; 0 or tag.count('&gt;') &gt; 0:\n        raise ValueError(\"No '&gt;' or '&lt;' allowed in get_xml tag name!\")\n    return rf\"&lt;{tag}&gt;(.*?)&lt;/{tag}&gt;\"\n</code></pre>"},{"location":"prompt/#prompt.grab","title":"<code>grab(tag, content)</code>","text":"<p>Alias for get_xml</p> Source code in <code>alana/prompt.py</code> <pre><code>def grab(tag: str, content: str) -&gt; List[str]:\n    \"\"\"Alias for get_xml\"\"\"\n    return get_xml(tag=tag, content=content)\n</code></pre>"},{"location":"prompt/#prompt.n_shot","title":"<code>n_shot(instruction, n_examples=5, model=globals.DEFAULT_MODEL, api_key=None, max_tokens=1024, temperature=0.3, **kwargs)</code>","text":"<p>Alias for gen_examples</p> Source code in <code>alana/prompt.py</code> <pre><code>def n_shot(instruction: str, n_examples: int = 5, model: str = globals.DEFAULT_MODEL, api_key: Optional[str] = None, max_tokens: int = 1024, temperature=0.3, **kwargs) -&gt; str:\n    \"\"\"Alias for gen_examples\"\"\"\n    return gen_examples(instruction=instruction, n_examples=n_examples, model=model, api_key=api_key, max_tokens=max_tokens, temperature=temperature, **kwargs)\n</code></pre>"},{"location":"prompt/#prompt.n_shot_list","title":"<code>n_shot_list(instruction, n_examples=5, model=globals.DEFAULT_MODEL, api_key=None, max_tokens=1024, temperature=0.3, **kwargs)</code>","text":"<p>Alias for gen_examples_list</p> Source code in <code>alana/prompt.py</code> <pre><code>def n_shot_list(instruction: str, n_examples: int = 5, model: str = globals.DEFAULT_MODEL, api_key: Optional[str] = None, max_tokens: int = 1024, temperature=0.3, **kwargs) -&gt; List[str]:\n    \"\"\"Alias for gen_examples_list\"\"\"\n    return gen_examples_list(instruction=instruction, n_examples=n_examples, model=model, api_key=api_key, max_tokens=max_tokens, temperature=temperature, **kwargs)\n</code></pre>"},{"location":"prompt/#prompt.pretty_print","title":"<code>pretty_print(var, loud=True, model='sonnet')</code>","text":"<p>Pretty-print an arbitrary variable. By default, uses Sonnet (not globals.DEFAULT_MODEL).</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>Any</code> <p>The variable to pretty-print.</p> required <code>loud</code> <code>bool</code> <p>Whether to print the pretty-printed output. Defaults to True.</p> <code>True</code> <code>model</code> <code>str</code> <p>The name of the model to use. Defaults to \"sonnet\".</p> <code>'sonnet'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The pretty-printed representation of the variable.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no  tags are found in the generated output.</p> Notes <ul> <li>The function constructs a system prompt using the <code>globals.SYSTEM[\"pretty_print\"]</code> template.</li> <li>The function constructs a user prompt using the <code>globals.USER[\"pretty_print\"]</code> template and the provided <code>var</code>.</li> <li>The function calls the <code>gen</code> function to generate the pretty-printed output based on the system prompt, user prompt, and specified <code>model</code>.</li> <li>The function uses the <code>get_xml</code> function to extract the content within the <code>&lt;pretty&gt;</code> tags from the generated output.</li> <li>If no <code>&lt;pretty/&gt;</code> tags are found in the model output, the function raises a <code>ValueError</code>.</li> <li>If multiple <code>&lt;pretty&gt;</code> tags are found in the model output, the function uses the last one as the pretty-printed output.</li> <li>The function returns the pretty-printed output as a string.</li> </ul> Example <p>my_var = {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"} pretty_output = pretty_print(my_var) {     \"name\": \"John\",     \"age\": 30,     \"city\": \"New York\" } print(pretty_output) {     \"name\": \"John\",     \"age\": 30,     \"city\": \"New York\" }</p> Source code in <code>alana/prompt.py</code> <pre><code>def pretty_print(var: Any, loud: bool = True, model: str = \"sonnet\") -&gt; str:\n    \"\"\"Pretty-print an arbitrary variable. By default, uses Sonnet (not globals.DEFAULT_MODEL).\n\n    Args:\n        var (Any): The variable to pretty-print.\n        loud (bool, optional): Whether to print the pretty-printed output. Defaults to True.\n        model (str, optional): The name of the model to use. Defaults to \"sonnet\".\n\n    Returns:\n        str: The pretty-printed representation of the variable.\n\n    Raises:\n        ValueError: If no &lt;pretty/&gt; tags are found in the generated output.\n\n    Notes:\n        - The function constructs a system prompt using the `globals.SYSTEM[\"pretty_print\"]` template.\n        - The function constructs a user prompt using the `globals.USER[\"pretty_print\"]` template and the provided `var`.\n        - The function calls the `gen` function to generate the pretty-printed output based on the system prompt, user prompt, and specified `model`.\n        - The function uses the `get_xml` function to extract the content within the `&lt;pretty&gt;` tags from the generated output.\n        - If no `&lt;pretty/&gt;` tags are found in the model output, the function raises a `ValueError`.\n        - If multiple `&lt;pretty&gt;` tags are found in the model output, the function uses the last one as the pretty-printed output.\n        - The function returns the pretty-printed output as a string.\n\n    Example:\n        &gt;&gt;&gt; my_var = {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}\n        &gt;&gt;&gt; pretty_output = pretty_print(my_var)\n        {\n            \"name\": \"John\",\n            \"age\": 30,\n            \"city\": \"New York\"\n        }\n        &gt;&gt;&gt; print(pretty_output)\n        {\n            \"name\": \"John\",\n            \"age\": 30,\n            \"city\": \"New York\"\n        }\n    \"\"\"\n    system = globals.SYSTEM[\"pretty_print\"]\n    user = globals.USER[\"pretty_print\"].format(var=f'{var}')\n\n    string: str = gen(user=user, system=system, model=model)\n    pretty: Union[List[str], str] = get_xml(tag=\"pretty\", content=string)\n    if len(pretty) == 0:\n        raise ValueError(\"`pretty_print`: XML parsing error! Number of &lt;pretty/&gt; tags is 0.\")\n    else:\n        pretty = pretty[-1]\n    if loud:\n        print(pretty)\n    return pretty\n</code></pre>"},{"location":"prompt/#prompt.remove_xml","title":"<code>remove_xml(tag='reasoning', content='', repl='')</code>","text":"<p>Return a copy of <code>content</code> with  XML elements (both content and tag) replaced with <code>repl</code> (default \"\").</p> Source code in <code>alana/prompt.py</code> <pre><code>def remove_xml(tag: str = \"reasoning\", content: str = \"\", repl: str=\"\") -&gt; str:\n    \"\"\"Return a copy of `content` with &lt;tag/&gt; XML elements (both content and tag) replaced with `repl` (default \"\").\"\"\"\n    if tag.count('&lt;') &gt; 0 or tag.count('&gt;') &gt; 0:\n        raise ValueError(\"No '&gt;' or '&lt;' allowed in get_xml tag name!\")\n    if content == \"\":\n        red(var=\"`remove_xml`: Empty string provided as `content`.\") # TODO: Improve error logging\n    pattern: str = rf\"&lt;{tag}&gt;.*?&lt;/{tag}&gt;\" # NOTE: Removed group matching, so can't use `get_xml_pattern`\n    output: str = re.sub(pattern=pattern, repl=repl, string=content, flags=re.DOTALL)\n    return output\n</code></pre>"},{"location":"prompt/#prompt.rm_xml","title":"<code>rm_xml(tag, content)</code>","text":"<p>Alias for remove_xml</p> Source code in <code>alana/prompt.py</code> <pre><code>def rm_xml(tag: str, content: str) -&gt; str:\n    \"\"\"Alias for remove_xml\"\"\"\n    return rm_xml(tag=tag, content=content)\n</code></pre>"},{"location":"prompt/#prompt.xml","title":"<code>xml(tag, content)</code>","text":"<p>Alias for get_xml</p> Source code in <code>alana/prompt.py</code> <pre><code>def xml(tag: str, content: str) -&gt; List[str]:\n    \"\"\"Alias for get_xml\"\"\"\n    return get_xml(tag=tag, content=content)\n</code></pre>"},{"location":"website/","title":"Documentation Website","text":"<p>This website is generated with <code>mkdocs</code>.</p> <p>The API reference is rendered from doc-strings.</p> <p>The source is available on GitHub.</p> <p>The website is served via GitHub Pages from a dedicated repository.</p> <p>To build and serve the website locally:</p> <pre><code>$ git clone https://github.com/alat-rights/alana-utils\n$ cd alana-utils\n$ mkdocs build\n$ mkdocs serve\n</code></pre> <p>The <code>alana-utils-docs</code> repository contains the <code>site</code> directory, which is auto-generated by <code>mkdocs</code> and excluded from <code>alana-utils</code>.</p>"}]}